---
title: "Excerise 2"
author: "Min Wang"
date: "2024-02-13"
output: pdf_document
---

##  Load data and packages

```{r}
library(kableExtra)
library(tidyverse) # loads dplyr, ggplot2, and others
library(readr) # more informative and easy way to import data
library(stringr) # to handle text elements
library(tidytext) # includes set of functions useful for manipulating text
library(quanteda) # includes functions to implement Lexicoder
library(textdata)
library(academictwitteR) # for fetching Twitter data

getwd()

tweets  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/sentanalysis/newstweets.rds?raw=true")))
```

## Inspect and filter data

```{r}
tweets <- tweets %>%
  select(user_username, text, created_at, user_name,
         retweet_count, like_count, quote_count) %>%
  rename(username = user_username,
         newspaper = user_name,
         tweet = text)
head(tweets)

# tidy the data
tidy_tweets <- tweets %>% 
  mutate(desc = tolower(tweet)) %>%
  unnest_tokens(word, desc) %>%
  filter(str_detect(word, "[a-z]"))

tidy_tweets <- tidy_tweets %>%
    filter(!word %in% stop_words$word)
```
## Question 1: 

## Tokenizations for eight dataset of each source of newspaper
```{r}
# unnesting each token
tidy_tweetsa <- tweets %>% 
  filter(newspaper == "The Mirror") %>%
  mutate(desc = tolower(tweet)) %>%
  unnest_tokens(word, desc) %>%
  filter(str_detect(word, "[a-z]"))

tidy_tweetsb <- tweets %>% 
  filter(newspaper == "The Times") %>%
  mutate(desc = tolower(tweet)) %>%
  unnest_tokens(word, desc) %>%
  filter(str_detect(word, "[a-z]"))

tidy_tweetsc <- tweets %>% 
  filter(newspaper == "The Sun") %>%
  mutate(desc = tolower(tweet)) %>%
  unnest_tokens(word, desc) %>%
  filter(str_detect(word, "[a-z]"))

tidy_tweetsd <- tweets %>% 
  filter(newspaper == "The Telegraph") %>%
  mutate(desc = tolower(tweet)) %>%
  unnest_tokens(word, desc) %>%
  filter(str_detect(word, "[a-z]"))

tidy_tweetse <- tweets %>% 
  filter(newspaper == "The Guardian") %>%
  mutate(desc = tolower(tweet)) %>%
  unnest_tokens(word, desc) %>%
  filter(str_detect(word, "[a-z]"))

tidy_tweetsf <- tweets %>% 
  filter(newspaper == "Metro") %>%
  mutate(desc = tolower(tweet)) %>%
  unnest_tokens(word, desc) %>%
  filter(str_detect(word, "[a-z]"))

tidy_tweetsg <- tweets %>% 
  filter(newspaper == "Daily Mail U.K.") %>%
  mutate(desc = tolower(tweet)) %>%
  unnest_tokens(word, desc) %>%
  filter(str_detect(word, "[a-z]"))

tidy_tweetsh <- tweets %>% 
  filter(newspaper == "Evening Standard") %>%
  mutate(desc = tolower(tweet)) %>%
  unnest_tokens(word, desc) %>%
  filter(str_detect(word, "[a-z]"))

```

# Removing stops words for each dataset
```{r}
# removing stops words for "The Mirror" dataset
tidy_tweetsa <- tidy_tweetsa %>%
    filter(!word %in% stop_words$word)
# removing stops words for "The Times" dataset
tidy_tweetsb <- tidy_tweetsb %>%
    filter(!word %in% stop_words$word)
# removing stops words for "The Sun" dataset
tidy_tweetsc <- tidy_tweetsc %>%
    filter(!word %in% stop_words$word)
# removing stops words for "The Telegraph" dataset
tidy_tweetsd <- tidy_tweetsd %>%
    filter(!word %in% stop_words$word)
# removing stops words for "The Guardian" dataset
tidy_tweetse <- tidy_tweetse %>%
    filter(!word %in% stop_words$word)
# removing stops words for "Metro" dataset
tidy_tweetsf <- tidy_tweetsf %>%
    filter(!word %in% stop_words$word)
# removing stops words for "Daily Mail U.K." dataset
tidy_tweetsg <- tidy_tweetsg %>%
    filter(!word %in% stop_words$word)
# removing stops words for "Evening Standard" dataset
tidy_tweetsh <- tidy_tweetsh %>%
    filter(!word %in% stop_words$word)
```

# Sentiment trends over time for eight newspaper sources
 Make sure the data are properly arranged in ascending order by date, then add column, which     we'll call "order," the use of which will become clear when we do the sentiment analysis.
```{r}
#gen data variable, order and format date
tidy_tweetsa$date <- as.Date(tidy_tweetsa$created_at)
tidy_tweetsa <- tidy_tweetsa %>%
  arrange(date)
tidy_tweetsa$order <- 1:nrow(tidy_tweetsa)


tidy_tweetsb$date <- as.Date(tidy_tweetsb$created_at)
tidy_tweetsb <- tidy_tweetsb %>%
  arrange(date)
tidy_tweetsb$order <- 1:nrow(tidy_tweetsb)


tidy_tweetsc$date <- as.Date(tidy_tweetsc$created_at)
tidy_tweetsc <- tidy_tweetsc %>%
  arrange(date)
tidy_tweetsc$order <- 1:nrow(tidy_tweetsc)


tidy_tweetsd$date <- as.Date(tidy_tweetsd$created_at)
tidy_tweetsd <- tidy_tweetsd %>%
  arrange(date)
tidy_tweetsd$order <- 1:nrow(tidy_tweetsd)


tidy_tweetse$date <- as.Date(tidy_tweetse$created_at)
tidy_tweetse <- tidy_tweetse %>%
  arrange(date)
tidy_tweetse$order <- 1:nrow(tidy_tweetse)


tidy_tweetsf$date <- as.Date(tidy_tweetsf$created_at)
tidy_tweetsf <- tidy_tweetsf %>%
  arrange(date)
tidy_tweetsf$order <- 1:nrow(tidy_tweetsf)


tidy_tweetsg$date <- as.Date(tidy_tweetsg$created_at)
tidy_tweetsg <- tidy_tweetsg %>%
  arrange(date)
tidy_tweetsg$order <- 1:nrow(tidy_tweetsg)


tidy_tweetsh$date <- as.Date(tidy_tweetsh$created_at)
tidy_tweetsh <- tidy_tweetsh %>%
  arrange(date)
tidy_tweetsh$order <- 1:nrow(tidy_tweetsh)


```


```{r}
#get tweet sentiment by date 
tweets_nrc_sentimenta <- tidy_tweetsa %>%
  inner_join(get_sentiments("nrc")) %>%
  count(date, index = order %/% 1000, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
  
tweets_nrc_sentimentb <- tidy_tweetsb %>%
  inner_join(get_sentiments("nrc")) %>%
  count(date, index = order %/% 1000, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

tweets_nrc_sentimentc <- tidy_tweetsc %>%
  inner_join(get_sentiments("nrc")) %>%
  count(date, index = order %/% 1000, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

tweets_nrc_sentimentd <- tidy_tweetsd %>%
  inner_join(get_sentiments("nrc")) %>%
  count(date, index = order %/% 1000, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

tweets_nrc_sentimente <- tidy_tweetse %>%
  inner_join(get_sentiments("nrc")) %>%
  count(date, index = order %/% 1000, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

tweets_nrc_sentimentf <- tidy_tweetsf %>%
  inner_join(get_sentiments("nrc")) %>%
  count(date, index = order %/% 1000, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

tweets_nrc_sentimentg <- tidy_tweetsg %>%
  inner_join(get_sentiments("nrc")) %>%
  count(date, index = order %/% 1000, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

tweets_nrc_sentimenth <- tidy_tweetsh %>%
  inner_join(get_sentiments("nrc")) %>%
  count(date, index = order %/% 1000, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
```

```{r}
# Create a new tibble
sentiment_comparison <- bind_rows(
  mutate(tweets_nrc_sentimenta, source = "The Mirror"),
  mutate(tweets_nrc_sentimentb, source = "The Times"),
  mutate(tweets_nrc_sentimentc, source = "The Sun"),
  mutate(tweets_nrc_sentimentd, source = "The Telegraph"),
  mutate(tweets_nrc_sentimente, source = "The Guardian"),
  mutate(tweets_nrc_sentimentf, source = "Metro"),
  mutate(tweets_nrc_sentimentg, source = "Daily Mail U.K."),
  mutate(tweets_nrc_sentimenth, source = "Evening Standard")
)

sentiment_comparison
```

```{r}
# plot the results
ggplot(sentiment_comparison, aes(x = date, y = sentiment, fill = source)) +
  geom_point(alpha=0.5) +
  geom_smooth(method= loess, alpha=0.15, aes(color = source)) +
  ylab("nrc sentiment") +
  labs(title = "Different Sentiment Dynamics")
```



## Question 2:
 Build own dictionary-based filter technique and plot the result
 I am interested in the prevalence of the crime-related words in the news.
```{r}
crime <- c('crime', 'theft', 'robbery', 'assault', 'murder', 'kidnapping', 'burglary', 'arson',
           'illegal', 'unlawful', 'knife', 'gun', 'arrest', 'conviction', 'sentence', 'prison',
           'jail', 'criminal', 'law', 'punishment')

#get total tweets per day (no missing dates so no date completion required)
tidy_tweets$date <- as.Date(tidy_tweets$created_at)
tidy_tweets <- tidy_tweets %>%
  arrange(date)
tidy_tweets$order <- 1:nrow(tidy_tweets)

totals <- tidy_tweets %>%
  mutate(obs=1) %>%
  group_by(date) %>%
  summarise(sum_words = sum(obs))

#plot
tidy_tweets %>%
  mutate(obs=1) %>%
  filter(grepl(paste0(crime, collapse = "|"),word, ignore.case = T)) %>%
  group_by(date) %>%
  summarise(sum_mwords = sum(obs)) %>%
  full_join(totals, word, by="date") %>%
  mutate(sum_mwords= ifelse(is.na(sum_mwords), 0, sum_mwords),
         pctmwords = sum_mwords/sum_words) %>%
  ggplot(aes(date, pctmwords)) +
  geom_point(alpha=0.5) +
  geom_smooth(method= loess, alpha=0.25) +
  xlab("Date") + ylab("% crime-related words")
```


## Question 3:
Apply the Lexicoder Sentiment Dictionary to the news tweets, but break down the analysis by newspaper.

```{r}
# use the `quanteda` package to create a "corpus" object
tweets$date <- as.Date(tweets$created_at)
tweet_corpus <- corpus(tweets, text_field = "tweet", docvars = "newspaper")

# tokenize the text using the `tokens()` function from quanteda, removing punctuation as well
toks_news <- tokens(tweet_corpus, remove_punct = TRUE)

# select only the "negative" and "positive" categories
data_dictionary_LSD2015_pos_neg <- data_dictionary_LSD2015[1:2]
toks_news_lsd <- tokens_lookup(toks_news, dictionary = data_dictionary_LSD2015_pos_neg)


# create a document document-feature matrix and group it by newspaper
dfmat_news_lsd <- dfm(toks_news_lsd) %>% 
  dfm_group(groups = newspaper) 


